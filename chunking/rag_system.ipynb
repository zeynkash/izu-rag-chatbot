{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464ea224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG SYSTEM SETUP\n",
      "======================================================================\n",
      "\n",
      "Loading chunks...\n",
      "‚úì Loaded 1747 chunks\n",
      "\n",
      "Sample chunk:\n",
      "{\n",
      "  \"chunk_id\": \"8704994871202097904_0\",\n",
      "  \"document_id\": 8704994871202097904,\n",
      "  \"chunk_index\": 0,\n",
      "  \"total_chunks\": 1,\n",
      "  \"content\": \"Title: Student Guide\\nURL: https://www.izu.edu.tr/en/international/international-students/student-guide\\n\\nStudent Guide Welcome to Istanbul Sabahattin Zaim Universit\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RAG SYSTEM SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load chunks\n",
    "print(\"\\nLoading chunks...\")\n",
    "with open('/home/zeynkash/projects/izu_scraper/chunking/chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(json.dumps(chunks[0], indent=2, ensure_ascii=False)[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a52ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding generation...\n",
      "‚úì Embedding generated!\n",
      "  Dimension: 1536\n",
      "  Sample values: [-0.02047703042626381, 0.05622651427984238, -0.013715344481170177, 0.01983712427318096, -0.02147955261170864]\n",
      "\n",
      "üí∞ Estimated cost: $0.02 for 1,021,515 tokens\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Get embedding from OpenAI API\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: OpenAI embedding model\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embedding\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Test on one chunk\n",
    "print(\"Testing embedding generation...\")\n",
    "test_embedding = get_embedding(chunks[0]['content'])\n",
    "print(f\"‚úì Embedding generated!\")\n",
    "print(f\"  Dimension: {len(test_embedding)}\")\n",
    "print(f\"  Sample values: {test_embedding[:5]}\")\n",
    "\n",
    "# Estimate cost\n",
    "total_tokens = sum(c['tokens'] for c in chunks)\n",
    "cost = (total_tokens / 1_000_000) * 0.02  # $0.02 per 1M tokens for text-embedding-3-small\n",
    "print(f\"\\nüí∞ Estimated cost: ${cost:.2f} for {total_tokens:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5dc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING EMBEDDINGS\n",
      "======================================================================\n",
      "Generating embeddings in batches of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:38<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Embeddings generated!\n",
      "  Shape: (1747, 1536)\n",
      "  Size: 10.24 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def generate_embeddings_batch(chunks, batch_size=100, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all chunks in batches\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk objects\n",
    "        batch_size: Number of chunks to process at once\n",
    "        model: OpenAI embedding model\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Generating embeddings in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing batches\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        texts = [chunk['content'].replace(\"\\n\", \" \") for chunk in batch]\n",
    "        \n",
    "        try:\n",
    "            # API call for batch\n",
    "            response = openai.embeddings.create(\n",
    "                input=texts,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Rate limit safety (optional)\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {i}-{i+batch_size}: {e}\")\n",
    "            # Retry with smaller batch or skip\n",
    "            continue\n",
    "    \n",
    "    return np.array(embeddings, dtype='float32')\n",
    "\n",
    "# Generate all embeddings\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "embeddings = generate_embeddings_batch(chunks, batch_size=100)\n",
    "\n",
    "print(f\"\\n‚úì Embeddings generated!\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Size: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97899d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved embeddings to: embeddings_openai.npy\n",
      "‚úì Saved metadata to: embedding_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Save embeddings\n",
    "np.save('embeddings_openai.npy', embeddings)\n",
    "print(\"‚úì Saved embeddings to: embeddings_openai.npy\")\n",
    "\n",
    "# Save embedding metadata\n",
    "embedding_metadata = {\n",
    "    'model': 'text-embedding-3-small',\n",
    "    'dimension': embeddings.shape[1],\n",
    "    'total_embeddings': embeddings.shape[0],\n",
    "    'date_created': pd.Timestamp.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open('embedding_metadata.json', 'w') as f:\n",
    "    json.dump(embedding_metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úì Saved metadata to: embedding_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf59372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILDING FAISS INDEX\n",
      "======================================================================\n",
      "Embeddings shape: (1747, 1536)\n",
      "\n",
      "‚úì FAISS index created!\n",
      "  Total vectors: 1747\n",
      "  Dimension: 1536\n",
      "‚úì Saved index to: faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING FAISS INDEX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load embeddings if needed\n",
    "embeddings = np.load('embeddings_openai.npy')\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)\n",
    "\n",
    "# Add embeddings\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"\\n‚úì FAISS index created!\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")\n",
    "print(f\"  Dimension: {dimension}\")\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(index, 'faiss_index.bin')\n",
    "print(\"‚úì Saved index to: faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d1bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING RETRIEVAL\n",
      "======================================================================\n",
      "Query: Y√ºksek lisans programlarƒ± neler?\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mY√ºksek lisans programlarƒ± neler?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m results = \u001b[43mretrieve_relevant_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results, \u001b[32m1\u001b[39m):\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mretrieve_relevant_chunks\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Generate query embedding\u001b[39;00m\n\u001b[32m     13\u001b[39m query_embedding = np.array([get_embedding(query)], dtype=\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mfaiss\u001b[49m.normalize_L2(query_embedding)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Search in FAISS\u001b[39;00m\n\u001b[32m     17\u001b[39m scores, indices = index.search(query_embedding, top_k)\n",
      "\u001b[31mNameError\u001b[39m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve most relevant chunks for a query\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        top_k: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant chunks with scores\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = np.array([get_embedding(query)], dtype='float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search in FAISS\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        chunk = chunks[idx]\n",
    "        results.append({\n",
    "            'score': float(score),\n",
    "            'content': chunk['content'],\n",
    "            'metadata': chunk['metadata'],\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'tokens': chunk['tokens']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING RETRIEVAL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_query = \"Y√ºksek lisans programlarƒ± neler?\"\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "results = retrieve_relevant_chunks(test_query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {result['score']:.4f}\")\n",
    "    print(f\"   Title: {result['metadata']['title']}\")\n",
    "    print(f\"   URL: {result['metadata']['url']}\")\n",
    "    print(f\"   Content: {result['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efdcffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "Sen ƒ∞stanbul Sabahattin Zaim √úniversitesi i√ßin bir yardƒ±mcƒ± asistansƒ±n. \n",
      "√ñƒürencilere ve ziyaret√ßilere √ºniversite hakkƒ±nda bilgi veriyorsun.\n",
      "\n",
      "√ñnemli kurallar:\n",
      "1. Sadece verilen CONTEXT bilgilerini kullan\n",
      "2. Bilmediƒüin bir ≈üey sorulursa, bilmediƒüini s√∂yle\n",
      "3. Nazik ve profesyonel ol\n",
      "4. Cevaplarƒ±nda kaynak URL'lerini belirt\n",
      "5. T√ºrk√ße karakter kullan (ƒ±, ƒü, √º, ≈ü, √∂, √ß)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "User Message:\n",
      "CONTEXT:\n",
      "\n",
      "[KAYNAK 1]\n",
      "Ba≈ülƒ±k: Diploma, Sertifika ve Diƒüer ƒ∞lgili Belgelerin D√ºzenlenmesine ƒ∞li≈ükin Y√∂nerge\n",
      "URL: https://www.izu.edu.tr/izu-hakkinda/mevzuat/yonergeler/diploma-sertifika-ve-di%C4%9Fer-i-lgili-belgelerin-d%C3%BCzenlenmesine-i-li%C5%9Fkin-y%C3%B6nerge\n",
      "ƒ∞√ßerik:\n",
      "Tanƒ±mlar 4- Bu Y√∂nergede ge√ßen; a) Dekan: √úniversitesinin Fak√ºlte Dekanƒ±nƒ±, b) Hologram: √úniversitesine ait bilgileri ta≈üƒ±yan etiketi, c) M√ºd√ºr: √úniversitesinin Y√ºksekokul Enstit√º M√ºd√ºr√ºn√º, √ß) Rekt√∂r: √úniversitesi Rekt√∂r√ºn√º, d) \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def create_rag_prompt(query, retrieved_chunks, language='auto'):\n",
    "    \"\"\"\n",
    "    Create prompt for OpenAI with retrieved context\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        retrieved_chunks: List of relevant chunks\n",
    "        language: 'tr', 'en', or 'auto'\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt\n",
    "    \"\"\"\n",
    "    # Detect language if auto\n",
    "    if language == 'auto':\n",
    "        # Simple detection: check for Turkish characters\n",
    "        if any(char in query for char in 'ƒü√º≈üƒ±√∂√ßƒû√ú≈ûƒ∞√ñ√á'):\n",
    "            language = 'tr'\n",
    "        else:\n",
    "            language = 'en'\n",
    "    \n",
    "    # System prompt\n",
    "    system_prompts = {\n",
    "        'tr': \"\"\"Sen ƒ∞stanbul Sabahattin Zaim √úniversitesi i√ßin bir yardƒ±mcƒ± asistansƒ±n. \n",
    "√ñƒürencilere ve ziyaret√ßilere √ºniversite hakkƒ±nda bilgi veriyorsun.\n",
    "\n",
    "√ñnemli kurallar:\n",
    "1. Sadece verilen CONTEXT bilgilerini kullan\n",
    "2. Bilmediƒüin bir ≈üey sorulursa, bilmediƒüini s√∂yle\n",
    "3. Nazik ve profesyonel ol\n",
    "4. Cevaplarƒ±nda kaynak URL'lerini belirt\n",
    "5. T√ºrk√ße karakter kullan (ƒ±, ƒü, √º, ≈ü, √∂, √ß)\"\"\",\n",
    "        \n",
    "        'en': \"\"\"You are an assistant for Istanbul Sabahattin Zaim University.\n",
    "You help students and visitors with information about the university.\n",
    "\n",
    "Important rules:\n",
    "1. Only use information from the provided CONTEXT\n",
    "2. If you don't know something, say you don't know\n",
    "3. Be polite and professional\n",
    "4. Include source URLs in your answers\n",
    "5. Keep answers concise and clear\n",
    "6. Detect the user‚Äôs question language.\n",
    "7. Always translate the question into English internally for searching and retrieval.\n",
    "8. Search Turkish data first.\n",
    "9. If an answer is found ‚Üí use it.\n",
    "10. If no answer exists in Turkish, search the English data.\n",
    "11. Regardless of the internal search language or source language, always answer in the user‚Äôs original language.\n",
    "12. If the final answer was sourced from English data but the user asked in Turkish, translate it into natural Turkish before responding.\n",
    "13. Never show the translation process to the user.\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Build context from chunks\n",
    "    context_parts = []\n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        context_parts.append(f\"\"\"\n",
    "[KAYNAK {i}]\n",
    "Ba≈ülƒ±k: {chunk['metadata']['title']}\n",
    "URL: {chunk['metadata']['url']}\n",
    "ƒ∞√ßerik:\n",
    "{chunk['content']}\n",
    "\"\"\" if language == 'tr' else f\"\"\"\n",
    "[SOURCE {i}]\n",
    "Title: {chunk['metadata']['title']}\n",
    "URL: {chunk['metadata']['url']}\n",
    "Content:\n",
    "{chunk['content']}\n",
    "\"\"\")\n",
    "    \n",
    "    context = \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    # User message\n",
    "    user_message = f\"\"\"CONTEXT:\n",
    "{context}\n",
    "\n",
    "SORU: {query}\n",
    "\n",
    "L√ºtfen yukarƒ±daki CONTEXT bilgilerini kullanarak soruyu yanƒ±tla. Hangi kaynaktan aldƒ±ƒüƒ±nƒ± belirt.\"\"\" if language == 'tr' else f\"\"\"CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Please answer the question using the CONTEXT above. Mention which source you used.\"\"\"\n",
    "    \n",
    "    return system_prompts[language], user_message\n",
    "\n",
    "# Test prompt creation\n",
    "system, user = create_rag_prompt(test_query, results[:3], language='tr')\n",
    "\n",
    "print(\"System Prompt:\")\n",
    "print(system)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nUser Message:\")\n",
    "print(user[:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b929a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING COMPLETE RAG PIPELINE\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "\n",
      "üìù Soru: Y√ºksek lisans ba≈üvurusu nasƒ±l yapƒ±lƒ±r?\n",
      "\n",
      "ü§ñ Cevap:\n",
      "Y√ºksek lisans ba≈üvurusu yapmak isteyen √∂ƒürencilerin en az d√∂rt yƒ±llƒ±k lisans diplomasƒ±na sahip olmalarƒ± ve mevzuatta belirtilen diƒüer ≈üartlarƒ± saƒülamalarƒ± gerekmektedir. T√ºrkiye vatanda≈üƒ± √∂ƒürencilerden d√∂rt yƒ±llƒ±k lisans diplomasƒ±nƒ± yurt dƒ±≈üƒ±ndaki yabancƒ± √ºniversitelerden almƒ±≈ü olanlarƒ±n diplomalarƒ±nƒ±n denkliƒüinin Y√ºksek√∂ƒüretim Kurulu tarafƒ±ndan onaylanmƒ±≈ü olmasƒ± gerekmektedir. \n",
      "\n",
      "Lisans√ºst√º programlara hakkƒ±nƒ± kazanan asƒ±l ve yedek adaylarƒ±n listesi enstit√º y√∂netim kurulu kararƒ± ile kesinle≈üir ve sonu√ßlar enstit√º m√ºd√ºrl√ºƒü√º tarafƒ±ndan ilan edilir. Kayƒ±tlar, akademik takvimde belirtilen g√ºnlerde yapƒ±lƒ±r. Kazanan adaylar, belirtilen s√ºre i√ßerisinde kesin kayƒ±tlarƒ±nƒ± yaptƒ±rmalƒ±dƒ±r. S√ºresi i√ßinde kayƒ±t yaptƒ±rmayan adaylar haklarƒ±nƒ± kaybeder ve bu adaylarƒ±n yerine ba≈üarƒ±lƒ± bulunan yedek adaylar genel ba≈üarƒ± deƒüerlendirme sƒ±rasƒ± esas alƒ±narak kabul edilebilir.\n",
      "\n",
      "Daha fazla bilgi i√ßin [Uzaktan √ñƒüretim Y√∂nergesi](https://izu.edu.tr/izu-hakkinda/mevzuat/yonergeler/uzaktan-%C3%B6%C4%9Fretim-y%C3%B6nergesi) kaynaƒüƒ±nƒ± inceleyebilirsiniz.\n",
      "\n",
      "üìö Kaynaklar:\n",
      "  1. Uzaktan √ñƒüretim Y√∂nergesi\n",
      "     https://izu.edu.tr/izu-hakkinda/mevzuat/yonergeler/uzaktan-%C3%B6%C4%9Fretim-y%C3%B6nergesi\n",
      "     Skor: 0.6188\n",
      "  2. Uzaktan √ñƒüretim Y√∂nergesi\n",
      "     https://www.izu.edu.tr/izu-hakkinda/mevzuat/yonergeler/uzaktan-%C3%B6%C4%9Fretim-y%C3%B6nergesi\n",
      "     Skor: 0.6188\n",
      "  3. Graduate Education Institute Application\n",
      "     https://www.izu.edu.tr/en/academics/institute/graduate-education-institute/graduate\n",
      "     Skor: 0.6103\n",
      "\n",
      "üí∞ Token kullanƒ±mƒ±: 2368\n"
     ]
    }
   ],
   "source": [
    "def answer_question(query, top_k=5, language='auto', model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve + Generate answer\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        language: 'tr', 'en', or 'auto'\n",
    "        model: OpenAI model to use\n",
    "    \n",
    "    Returns:\n",
    "        dict with answer and sources\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(f\"üîç Retrieving relevant information...\")\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query, top_k)\n",
    "    \n",
    "    # Step 2: Create prompt\n",
    "    system_prompt, user_message = create_rag_prompt(query, retrieved_chunks, language)\n",
    "    \n",
    "    # Step 3: Generate answer with OpenAI\n",
    "    print(f\"ü§ñ Generating answer with {model}...\")\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.3,  # Lower = more factual\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Format output\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'sources': [\n",
    "            {\n",
    "                'title': chunk['metadata']['title'],\n",
    "                'url': chunk['metadata']['url'],\n",
    "                'relevance_score': chunk['score']\n",
    "            }\n",
    "            for chunk in retrieved_chunks\n",
    "        ],\n",
    "        'model': model,\n",
    "        'total_tokens': response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Test complete RAG\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING COMPLETE RAG PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = answer_question(\n",
    "    \"Y√ºksek lisans ba≈üvurusu nasƒ±l yapƒ±lƒ±r?\",\n",
    "    top_k=3,\n",
    "    language='tr'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Soru: {result['query']}\")\n",
    "print(f\"\\nü§ñ Cevap:\\n{result['answer']}\")\n",
    "print(f\"\\nüìö Kaynaklar:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  {i}. {source['title']}\")\n",
    "    print(f\"     {source['url']}\")\n",
    "    print(f\"     Skor: {source['relevance_score']:.4f}\")\n",
    "print(f\"\\nüí∞ Token kullanƒ±mƒ±: {result['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8670de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING MULTIPLE QUESTIONS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Test 1/5\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù Y√ºksek lisans programlarƒ± neler?\n",
      "\n",
      "ü§ñ √úzg√ºn√ºm, ancak yukarƒ±daki CONTEXT bilgileri arasƒ±nda ƒ∞stanbul Sabahattin Zaim √úniversitesi'nin y√ºksek lisans programlarƒ± hakkƒ±nda spesifik bir bilgi bulunmamaktadƒ±r. Daha fazla bilgi almak i√ßin √ºniversitenin resmi web sitesini ziyaret etmenizi √∂neririm. \n",
      "\n",
      "Kaynak: [Graduate Education Institute Application](https://www.izu.edu.tr/en/academics/institute/graduate-education-institute/graduate)\n",
      "\n",
      "üí∞ Tokens: 2021\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 2/5\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù √úniversite √ºcretleri ne kadar?\n",
      "\n",
      "ü§ñ √úniversite √ºcretleri her yƒ±l M√ºtevelli Heyeti tarafƒ±ndan maliyet artƒ±≈ülarƒ± dikkate alƒ±narak yeniden belirlenmektedir. ƒ∞lan edilen √ºcretler yalnƒ±zca eƒüitim-√∂ƒüretim masraflarƒ±nƒ± kapsamaktadƒ±r; konaklama, beslenme, ula≈üƒ±m, kitap gibi diƒüer giderler bu √ºcretlerin dƒ±≈üƒ±nda kalmaktadƒ±r. Kesin √ºcret bilgileri i√ßin √ºniversitenin resmi web sitesini ziyaret etmenizi √∂neririm.\n",
      "\n",
      "Daha fazla bilgi i√ßin l√ºtfen ≈üu kaynaƒüa g√∂z atƒ±n: [√úcretler & √ñdemeler](https://www.izu.edu.tr/ogrenci/kayit-kabul/ucretler-odemeler).\n",
      "\n",
      "üí∞ Tokens: 2190\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 3/5\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù What are the tuition fees for international students?\n",
      "\n",
      "ü§ñ The tuition fees for international students at Istanbul Sabahattin Zaim University are as follows:\n",
      "\n",
      "**PhD Programs:**\n",
      "- General: $12,000\n",
      "- Clinical Psychology PhD: $22,500\n",
      "\n",
      "**Master‚Äôs Programs:**\n",
      "- Thesis: $9,000\n",
      "- Clinical Psychology (Thesis): $20,000\n",
      "- Non-Thesis: $8,000\n",
      "- Clinical Psychology (Non-Thesis): $17,000\n",
      "\n",
      "These fees are applicable for the normal study periods specified (4 years for PhD, 2 years for Master's Thesis, and 1.5 years for Master's Non-Thesis).\n",
      "\n",
      "For more details, you can visit the source: [Fees and Payment Methods](https://www.izu.edu.tr/en/academics/institute/graduate-education-institute/fees-scholarship).\n",
      "\n",
      "üí∞ Tokens: 1915\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 4/5\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù Burs imkanlarƒ± var mƒ±?\n",
      "\n",
      "ü§ñ Evet, ƒ∞stanbul Sabahattin Zaim √úniversitesi'nde burs imkanlarƒ± bulunmaktadƒ±r. Burslardan ve indirimlerden yararlanma ≈üartlarƒ±, belirli kriterleri ta≈üƒ±yan √∂ƒürenciler i√ßin ge√ßerlidir. √ñƒürenciler, burs ve indirimleri a≈üaƒüƒ±daki sƒ±raya g√∂re k√ºm√ºlatif olarak alabilirler: \n",
      "\n",
      "1. Kazandƒ±ƒüƒ± ƒ∞ndirim\n",
      "2. Varsa Tercih ƒ∞ndirimi\n",
      "3. Varsa Ba≈üarƒ± Bursu, Spor/Sanat Burslarƒ± ve diƒüer indirimler\n",
      "\n",
      "Ayrƒ±ca, karde≈ü ve e≈ü indirimleri, ≈üehit ve gazi burslarƒ± gibi √∂zel d√ºzenlemeler de mevcuttur. Bu burslar, diƒüer indirimlerden baƒüƒ±msƒ±z olarak saƒülanmaktadƒ±r. \n",
      "\n",
      "Daha fazla bilgi i√ßin burs y√∂nergesine [buradan](https://izu.edu.tr/izu-hakkinda/mevzuat/yonergeler/burs-yonergesi) ula≈üabilirsiniz.\n",
      "\n",
      "üí∞ Tokens: 2175\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 5/5\n",
      "======================================================================\n",
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù Kamp√ºste yurt var mƒ±?\n",
      "\n",
      "ü§ñ Evet, ƒ∞stanbul Sabahattin Zaim √úniversitesi kamp√ºs√ºnde konukevleri bulunmaktadƒ±r. Bu konukevleri, kƒ±z ve erkek √∂ƒürencilerin g√ºvenli ve konforlu bir ≈üekilde barƒ±nmalarƒ±nƒ± saƒülamanƒ±n yanƒ± sƒ±ra, ki≈üisel ve sosyal geli≈üimlerine katkƒ± saƒülayacak bir√ßok faaliyet d√ºzenlemektedir. Konukevlerinde sinema salonlarƒ±, kafeteryalar, fitness salonlarƒ±, el sanatlarƒ± at√∂lyeleri ve bilgisayar laboratuvarlarƒ± gibi olanaklar da mevcuttur. \n",
      "\n",
      "Daha fazla bilgi i√ßin [bu kaynaƒüa](https://izu.edu.tr/ogrenci/izu-de-yasam) g√∂z atabilirsiniz.\n",
      "\n",
      "üí∞ Tokens: 1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Y√ºksek lisans programlarƒ± neler?\",\n",
    "    \"√úniversite √ºcretleri ne kadar?\",\n",
    "    \"What are the tuition fees for international students?\",\n",
    "    \"Burs imkanlarƒ± var mƒ±?\",\n",
    "    \"Kamp√ºste yurt var mƒ±?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MULTIPLE QUESTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Test {i}/{len(test_questions)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = answer_question(question, top_k=3)\n",
    "    \n",
    "    print(f\"üìù {result['query']}\")\n",
    "    print(f\"\\nü§ñ {result['answer']}\")\n",
    "    print(f\"\\nüí∞ Tokens: {result['total_tokens']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c0a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING CHATBOT WITH MEMORY\n",
      "======================================================================\n",
      "\n",
      "üë§ Kullanƒ±cƒ±: √ºniversitenin adƒ± nedir?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_relevant_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müë§ Kullanƒ±cƒ±: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     result = \u001b[43mchatbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mü§ñ Asistan: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müí∞ Tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mRAGChatbot.chat\u001b[39m\u001b[34m(self, query, top_k, language)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chat with memory\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Retrieve chunks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m retrieved_chunks = \u001b[43mretrieve_relevant_chunks\u001b[49m(query, top_k)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Create context\u001b[39;00m\n\u001b[32m     16\u001b[39m system_prompt, _ = create_rag_prompt(query, retrieved_chunks, language)\n",
      "\u001b[31mNameError\u001b[39m: name 'retrieve_relevant_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "class RAGChatbot:\n",
    "    \"\"\"\n",
    "    RAG Chatbot with conversation memory\n",
    "    \"\"\"\n",
    "    def __init__(self, model='gpt-4o-mini'):\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def chat(self, query, top_k=5, language='auto'):\n",
    "        \"\"\"Chat with memory\"\"\"\n",
    "        \n",
    "        # Retrieve chunks\n",
    "        retrieved_chunks = retrieve_relevant_chunks(query, top_k)\n",
    "        \n",
    "        # Create context\n",
    "        system_prompt, _ = create_rag_prompt(query, retrieved_chunks, language)\n",
    "        \n",
    "        # Build messages with history\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        \n",
    "        # Add conversation history (last 3 turns)\n",
    "        messages.extend(self.conversation_history[-6:])\n",
    "        \n",
    "        # Add current query\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Generate response\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Update history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': retrieved_chunks,\n",
    "            'tokens': response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Test chatbot with memory\n",
    "chatbot = RAGChatbot()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING CHATBOT WITH MEMORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Conversation\n",
    "questions = [\n",
    "    \"√ºniversitenin adƒ± nedir?\",\n",
    "    \"konumu nerde?\",  # Follow-up question\n",
    "    \"nasƒ±l gidebilirim?\",  # Another follow-up\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nüë§ Kullanƒ±cƒ±: {q}\")\n",
    "    result = chatbot.chat(q, top_k=3, language='tr')\n",
    "    print(f\"ü§ñ Asistan: {result['answer']}\")\n",
    "    print(f\"üí∞ Tokens: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "339d8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved RAG configuration to: rag_config.json\n",
      "\n",
      "======================================================================\n",
      "RAG SYSTEM COMPLETE! üéâ\n",
      "======================================================================\n",
      "\n",
      "Files created:\n",
      "  1. embeddings_openai.npy - OpenAI embeddings\n",
      "  2. faiss_index.bin - Vector index\n",
      "  3. rag_config.json - System configuration\n",
      "\n",
      "You can now:\n",
      "  - Use answer_question() for single queries\n",
      "  - Use RAGChatbot() for conversations\n",
      "  - Build a web interface (Streamlit/Gradio)\n"
     ]
    }
   ],
   "source": [
    "# Save complete configuration\n",
    "rag_config = {\n",
    "    'embedding_model': 'text-embedding-3-small',\n",
    "    'embedding_dimension': embeddings.shape[1],\n",
    "    'llm_model': 'gpt-4o-mini',\n",
    "    'total_chunks': len(chunks),\n",
    "    'faiss_index_path': 'faiss_index.bin',\n",
    "    'chunks_path': 'chunks.json',\n",
    "    'embeddings_path': 'embeddings_openai.npy',\n",
    "    'default_top_k': 5,\n",
    "    'temperature': 0.3,\n",
    "    'max_tokens': 1000,\n",
    "}\n",
    "\n",
    "with open('rag_config.json', 'w') as f:\n",
    "    json.dump(rag_config, f, indent=2)\n",
    "\n",
    "print(\"‚úì Saved RAG configuration to: rag_config.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAG SYSTEM COMPLETE! üéâ\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  1. embeddings_openai.npy - OpenAI embeddings\")\n",
    "print(\"  2. faiss_index.bin - Vector index\")\n",
    "print(\"  3. rag_config.json - System configuration\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  - Use answer_question() for single queries\")\n",
    "print(\"  - Use RAGChatbot() for conversations\")\n",
    "print(\"  - Build a web interface (Streamlit/Gradio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e752527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGChatbot at 0x7f62aee0cc50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAGChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126848cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created rag_system.py for production use\n"
     ]
    }
   ],
   "source": [
    "# Create a simple module for easy import\n",
    "production_code = '''\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load config\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Load resources\n",
    "with open('chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "index = faiss.read_index('faiss_index.bin')\n",
    "\n",
    "with open('rag_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "def answer_question(query, top_k=5):\n",
    "    \"\"\"Production RAG function\"\"\"\n",
    "    # Your answer_question code here\n",
    "    pass\n",
    "\n",
    "# Ready to use!\n",
    "'''\n",
    "\n",
    "with open('rag_system.py', 'w') as f:\n",
    "    f.write(production_code)\n",
    "\n",
    "print(\"‚úì Created rag_system.py for production use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1ba24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieving relevant information...\n",
      "ü§ñ Generating answer with gpt-4o-mini...\n",
      "üìù ƒ∞stanbul Sabahattin Zaim √úniversitesi‚Äônin yƒ±llƒ±k lisans √∂ƒürenim √ºcreti ne kadar?\n",
      "\n",
      "ü§ñ √úzg√ºn√ºm, ƒ∞stanbul Sabahattin Zaim √úniversitesi'nin yƒ±llƒ±k lisans √∂ƒürenim √ºcreti hakkƒ±nda elimde bilgi bulunmamaktadƒ±r. Ancak lisans√ºst√º eƒüitim √ºcretleri ve burslar hakkƒ±nda detaylƒ± bilgiye [bu linkten](https://www.izu.edu.tr/akademik/enstitu/lisansustu-egitim-enstitusu/ucretler-burs) ula≈üabilirsiniz. Ba≈üka bir konuda yardƒ±mcƒ± olmamƒ± ister misiniz?\n"
     ]
    }
   ],
   "source": [
    "result = answer_question(\"ƒ∞stanbul Sabahattin Zaim √úniversitesi‚Äônin yƒ±llƒ±k lisans √∂ƒürenim √ºcreti ne kadar?\", top_k=3)\n",
    "    \n",
    "print(f\"üìù {result['query']}\")\n",
    "print(f\"\\nü§ñ {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c8ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
