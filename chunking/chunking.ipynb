{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f6e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "‚úì Loaded 723 documents\n",
      "\n",
      "Columns: ['url', 'title', 'language', 'content']\n",
      "\n",
      "Sample document:\n",
      "{\n",
      "  \"url\": \"https://www.izu.edu.tr/en/international/international-students/student-guide\",\n",
      "  \"title\": \"Student Guide\",\n",
      "  \"language\": \"en\",\n",
      "  \"content\": \"Student Guide Welcome to Istanbul Sabahattin Zaim University (IZU) We are delighted to welcome all international students to IZU. To ensure a smooth transition into university life, we highly recommend reviewing International Student Guide. This guide contains essential information, and students are expected to familiarise themselves with its co\n",
      "\n",
      "Content length statistics:\n",
      "count      723.000000\n",
      "mean      3750.058091\n",
      "std       5305.768931\n",
      "min        107.000000\n",
      "25%        799.000000\n",
      "50%       1696.000000\n",
      "75%       3809.000000\n",
      "max      30185.000000\n",
      "Name: content_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load JSONL file\n",
    "data = []\n",
    "with open('/home/zeynkash/projects/izu_scraper/chunking/all_data_cleaned.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úì Loaded {len(data)} documents\")\n",
    "\n",
    "# Convert to DataFrame for easy exploration\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(json.dumps(data[0], indent=2, ensure_ascii=False)[:500])\n",
    "\n",
    "# Check content lengths\n",
    "df['content_length'] = df['content'].str.len()\n",
    "print(f\"\\nContent length statistics:\")\n",
    "print(df['content_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e309f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì tiktoken already installed\n",
      "\n",
      "Sample text:\n",
      "  Characters: 1093\n",
      "  Tokens: 231\n",
      "  Ratio: ~4.7 chars per token\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    print(\"‚úì tiktoken already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing tiktoken...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tiktoken\"])\n",
    "    import tiktoken\n",
    "    print(\"‚úì tiktoken installed\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test token counting\n",
    "sample_text = data[0]['content']\n",
    "tokens = count_tokens(sample_text)\n",
    "chars = len(sample_text)\n",
    "\n",
    "print(f\"\\nSample text:\")\n",
    "print(f\"  Characters: {chars}\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Ratio: ~{chars/tokens:.1f} chars per token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134d7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Chunking:\n",
      "Original tokens: 231\n",
      "Number of chunks: 1\n",
      "\n",
      "First chunk (231 tokens):\n",
      "Student Guide Welcome to Istanbul Sabahattin Zaim University (IZU) We are delighted to welcome all international students to IZU. To ensure a smooth transition into university life, we highly recommend reviewing International Student Guide. This guide contains essential information, and students are\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(text, chunk_size=800, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Split text into chunks with overlap\n",
    "    \n",
    "    Args:\n",
    "        text: Text to split\n",
    "        chunk_size: Target chunk size in tokens\n",
    "        chunk_overlap: Overlap between chunks in tokens\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Encode text to tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # If text is shorter than chunk_size, return as single chunk\n",
    "    if len(tokens) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        # Get chunk of tokens\n",
    "        end = start + chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Decode back to text\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start = end - chunk_overlap\n",
    "        \n",
    "        # Prevent infinite loop\n",
    "        if start >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test on sample\n",
    "test_text = data[0]['content']\n",
    "test_chunks = split_into_chunks(test_text, chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "print(f\"\\nTest Chunking:\")\n",
    "print(f\"Original tokens: {count_tokens(test_text)}\")\n",
    "print(f\"Number of chunks: {len(test_chunks)}\")\n",
    "print(f\"\\nFirst chunk ({count_tokens(test_chunks[0])} tokens):\")\n",
    "print(test_chunks[0][:300])\n",
    "print(\"\\n...\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"\\nSecond chunk ({count_tokens(test_chunks[1])} tokens):\")\n",
    "    print(test_chunks[1][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8426c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smart Chunking Test:\n",
      "Original tokens: 231\n",
      "Number of chunks: 1\n",
      "\n",
      "First chunk (223 tokens):\n",
      "Student Guide Welcome to Istanbul Sabahattin Zaim University (IZU) We are delighted to welcome all international students to IZU To ensure a smooth transition into university life, we highly recommend reviewing International Student Guide This guide contains essential information, and students are e\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def smart_split_into_chunks(text, chunk_size=800, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Split text into chunks with sentence boundary awareness\n",
    "    \n",
    "    Args:\n",
    "        text: Text to split\n",
    "        chunk_size: Target chunk size in tokens\n",
    "        chunk_overlap: Overlap between chunks in tokens\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Split into sentences (works for both Turkish and English)\n",
    "    sentence_endings = r'[.!?]\\s+'\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        # If single sentence exceeds chunk_size, split it by tokens\n",
    "        if sentence_tokens > chunk_size:\n",
    "            # Save current chunk if not empty\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "            \n",
    "            # Split long sentence\n",
    "            tokens = encoding.encode(sentence)\n",
    "            for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "                chunk_tokens = tokens[i:i + chunk_size]\n",
    "                chunks.append(encoding.decode(chunk_tokens))\n",
    "            continue\n",
    "        \n",
    "        # Check if adding this sentence exceeds chunk_size\n",
    "        if current_tokens + sentence_tokens > chunk_size:\n",
    "            # Save current chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "            # Start new chunk with overlap\n",
    "            # Keep last few sentences for context\n",
    "            overlap_sentences = []\n",
    "            overlap_tokens = 0\n",
    "            \n",
    "            for sent in reversed(current_chunk):\n",
    "                sent_tokens = count_tokens(sent)\n",
    "                if overlap_tokens + sent_tokens <= chunk_overlap:\n",
    "                    overlap_sentences.insert(0, sent)\n",
    "                    overlap_tokens += sent_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            current_chunk = overlap_sentences\n",
    "            current_tokens = overlap_tokens\n",
    "        \n",
    "        # Add sentence to current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_tokens += sentence_tokens\n",
    "    \n",
    "    # Add last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test smart chunking\n",
    "test_chunks_smart = smart_split_into_chunks(test_text, chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "print(f\"\\nSmart Chunking Test:\")\n",
    "print(f\"Original tokens: {count_tokens(test_text)}\")\n",
    "print(f\"Number of chunks: {len(test_chunks_smart)}\")\n",
    "print(f\"\\nFirst chunk ({count_tokens(test_chunks_smart[0])} tokens):\")\n",
    "print(test_chunks_smart[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f89623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHUNKING ALL DOCUMENTS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 723/723 [00:00<00:00, 782.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunking complete!\n",
      "  Original documents: 723\n",
      "  Total chunks: 1747\n",
      "  Average chunks per doc: 2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_document(doc, chunk_size=800, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Chunk a single document and return chunks with metadata\n",
    "    \n",
    "    Args:\n",
    "        doc: Document dict from JSONL\n",
    "        chunk_size: Target chunk size in tokens\n",
    "        chunk_overlap: Overlap in tokens\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk objects with metadata\n",
    "    \"\"\"\n",
    "    content = doc.get('content', '')\n",
    "    \n",
    "    # Add context header to content\n",
    "    title = doc.get('title', 'Untitled')\n",
    "    url = doc.get('url', '')\n",
    "    header = f\"Title: {title}\\nURL: {url}\\n\\n\"\n",
    "    \n",
    "    full_content = header + content\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_chunks = smart_split_into_chunks(full_content, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Create chunk objects with metadata\n",
    "    chunk_objects = []\n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        chunk_obj = {\n",
    "            'chunk_id': f\"{doc.get('id', hash(url))}_{i}\",\n",
    "            'document_id': doc.get('id', hash(url)),\n",
    "            'chunk_index': i,\n",
    "            'total_chunks': len(text_chunks),\n",
    "            'content': chunk_text,\n",
    "            'tokens': count_tokens(chunk_text),\n",
    "            'metadata': {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'language': doc.get('language', 'unknown'),\n",
    "                'section': doc.get('section', 'general'),\n",
    "                'date_scraped': doc.get('date_scraped', ''),\n",
    "            }\n",
    "        }\n",
    "        chunk_objects.append(chunk_obj)\n",
    "    \n",
    "    return chunk_objects\n",
    "\n",
    "# Chunk all documents\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING ALL DOCUMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for doc in tqdm(data, desc=\"Chunking documents\"):\n",
    "    try:\n",
    "        chunks = chunk_document(doc, chunk_size=800, chunk_overlap=150)\n",
    "        all_chunks.extend(chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error chunking document {doc.get('url', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úì Chunking complete!\")\n",
    "print(f\"  Original documents: {len(data)}\")\n",
    "print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "print(f\"  Average chunks per doc: {len(all_chunks) / len(data):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d09d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHUNK STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Token Distribution:\n",
      "  Mean: 585 tokens\n",
      "  Median: 696 tokens\n",
      "  Min: 20 tokens\n",
      "  Max: 914 tokens\n",
      "  Std Dev: 223 tokens\n",
      "\n",
      "Chunks per Document:\n",
      "  Mean: 2.4\n",
      "  Median: 1\n",
      "  Min: 1\n",
      "  Max: 20\n",
      "\n",
      "Language Distribution:\n",
      "  en: 545 chunks (31.2%)\n",
      "  tr: 1202 chunks (68.8%)\n",
      "\n",
      "Section Distribution (Top 5):\n",
      "  general: 1747 chunks (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# Calculate statistics\n",
    "chunk_tokens = [c['tokens'] for c in all_chunks]\n",
    "chunks_per_doc = {}\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    doc_id = chunk['document_id']\n",
    "    chunks_per_doc[doc_id] = chunks_per_doc.get(doc_id, 0) + 1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNK STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nToken Distribution:\")\n",
    "print(f\"  Mean: {statistics.mean(chunk_tokens):.0f} tokens\")\n",
    "print(f\"  Median: {statistics.median(chunk_tokens):.0f} tokens\")\n",
    "print(f\"  Min: {min(chunk_tokens)} tokens\")\n",
    "print(f\"  Max: {max(chunk_tokens)} tokens\")\n",
    "print(f\"  Std Dev: {statistics.stdev(chunk_tokens):.0f} tokens\")\n",
    "\n",
    "print(f\"\\nChunks per Document:\")\n",
    "print(f\"  Mean: {statistics.mean(chunks_per_doc.values()):.1f}\")\n",
    "print(f\"  Median: {statistics.median(chunks_per_doc.values()):.0f}\")\n",
    "print(f\"  Min: {min(chunks_per_doc.values())}\")\n",
    "print(f\"  Max: {max(chunks_per_doc.values())}\")\n",
    "\n",
    "# Language distribution\n",
    "lang_dist = {}\n",
    "for chunk in all_chunks:\n",
    "    lang = chunk['metadata']['language']\n",
    "    lang_dist[lang] = lang_dist.get(lang, 0) + 1\n",
    "\n",
    "print(f\"\\nLanguage Distribution:\")\n",
    "for lang, count in sorted(lang_dist.items()):\n",
    "    print(f\"  {lang}: {count} chunks ({count/len(all_chunks)*100:.1f}%)\")\n",
    "\n",
    "# Section distribution\n",
    "section_dist = {}\n",
    "for chunk in all_chunks:\n",
    "    section = chunk['metadata']['section']\n",
    "    section_dist[section] = section_dist.get(section, 0) + 1\n",
    "\n",
    "print(f\"\\nSection Distribution (Top 5):\")\n",
    "for section, count in sorted(section_dist.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {section}: {count} chunks ({count/len(all_chunks)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32918586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUALITY CHECK - RANDOM SAMPLES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Sample 1\n",
      "======================================================================\n",
      "Chunk ID: -6701365950551010967_0\n",
      "Chunk: 1/1\n",
      "Tokens: 192\n",
      "Language: en\n",
      "Section: general\n",
      "Title: ƒ∞Z√ú Hosts the Leaders of the Future\n",
      "\n",
      "Content:\n",
      "Title: ƒ∞Z√ú Hosts the Leaders of the Future\n",
      "URL: https://www.izu.edu.tr/en/news/2025/05/05/i-z%C3%BC-hosts-the-leaders-of-the-future\n",
      "\n",
      "ƒ∞Z√ú Hosts the Leaders of the Future 05.05.2025 The MUN (Model United Nations) event, whose opening session was held at Ba≈üak≈üehir Mehmet Emin Sara√ß Anatolian Imam Hatip High School, is continuing at the ƒ∞Z√ú Campus In the program, which began with an opening speech by\n",
      "...\n",
      "\n",
      "======================================================================\n",
      "Sample 2\n",
      "======================================================================\n",
      "Chunk ID: 2573411217997351349_6\n",
      "Chunk: 7/7\n",
      "Tokens: 636\n",
      "Language: tr\n",
      "Section: general\n",
      "Title: Baƒüƒ±l Deƒüerlendirme Y√∂nergesi\n",
      "\n",
      "Content:\n",
      "3) √ñƒürencilerin Ham Ba≈üarƒ± Notu hesaplanƒ±rken; virg√ºlden sonraki √º√ß√ºnc√º basamak be≈ü veya daha b√ºy√ºkse ikinci basamak artƒ±rƒ±lƒ±r, √º√ß√ºnc√º basamak d√∂rt veya daha k√º√ß√ºkse ikinci basamak deƒüi≈ütirilmeden bƒ±rakƒ±lƒ±r 4) Sƒ±nƒ±f ortalamasƒ±, Standart Sapma, T Skor, Z Skor hesaplanƒ±rken; virg√ºlden sonraki √º√ß√ºnc√º basamak be≈ü veya daha b√ºy√ºkse ikinci basamak artƒ±rƒ±lƒ±r, √º√ß√ºnc√º basamak d√∂rt veya daha k√º√ß√ºkse ikinci \n",
      "...\n",
      "\n",
      "======================================================================\n",
      "Sample 3\n",
      "======================================================================\n",
      "Chunk ID: 3735701065252947717_0\n",
      "Chunk: 1/1\n",
      "Tokens: 88\n",
      "Language: en\n",
      "Section: general\n",
      "Title: Announcements\n",
      "\n",
      "Content:\n",
      "Title: Announcements\n",
      "URL: https://izu.edu.tr/en/research/library/announcements\n",
      "\n",
      "Research and Teaching Assistant Job Announcement 26 July 2024 About Library working hours due to Eid al-Adha public holiday 21 June 2024 Library working hours changed until the university's education start date 16 February 2023 IZU Central Library, The Greatest Library in Istanbul has been opened 20 May 2018\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUALITY CHECK - RANDOM SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show 3 random chunks\n",
    "for i in range(3):\n",
    "    chunk = random.choice(all_chunks)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Chunk ID: {chunk['chunk_id']}\")\n",
    "    print(f\"Chunk: {chunk['chunk_index'] + 1}/{chunk['total_chunks']}\")\n",
    "    print(f\"Tokens: {chunk['tokens']}\")\n",
    "    print(f\"Language: {chunk['metadata']['language']}\")\n",
    "    print(f\"Section: {chunk['metadata']['section']}\")\n",
    "    print(f\"Title: {chunk['metadata']['title']}\")\n",
    "    print(f\"\\nContent:\")\n",
    "    print(chunk['content'][:400])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904cf6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ISSUE CHECKS\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  Chunks < 100 tokens: 28 (1.6%)\n",
      "  Sample short chunks:\n",
      "    - 91 tokens:  PANSTWOWA IM PAPIEZA JANA PAWLA II W BIALEJ PODLASKIEJ PANEVEZIO VIESOJI ISTAIGA EKONOMII I INNOWAC...\n",
      "    - 90 tokens: Title: Announcements\n",
      "URL: https://www.izu.edu.tr/en/research/library/announcements\n",
      "\n",
      "Research and Tea...\n",
      "    - 88 tokens: Title: Announcements\n",
      "URL: https://izu.edu.tr/en/research/library/announcements\n",
      "\n",
      "Research and Teachin...\n",
      "\n",
      "‚ö†Ô∏è  Chunks > 1000 tokens: 0 (0.0%)\n",
      "\n",
      "‚ö†Ô∏è  Empty chunks: 0\n",
      "\n",
      "‚úì Optimal chunks (300-900 tokens): 1455 (83.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ISSUE CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Very short chunks\n",
    "short_chunks = [c for c in all_chunks if c['tokens'] < 100]\n",
    "print(f\"\\n‚ö†Ô∏è  Chunks < 100 tokens: {len(short_chunks)} ({len(short_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "\n",
    "if short_chunks:\n",
    "    print(\"  Sample short chunks:\")\n",
    "    for chunk in short_chunks[:3]:\n",
    "        print(f\"    - {chunk['tokens']} tokens: {chunk['content'][:100]}...\")\n",
    "\n",
    "# Very long chunks\n",
    "long_chunks = [c for c in all_chunks if c['tokens'] > 1000]\n",
    "print(f\"\\n‚ö†Ô∏è  Chunks > 1000 tokens: {len(long_chunks)} ({len(long_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "\n",
    "if long_chunks:\n",
    "    print(\"  Sample long chunks:\")\n",
    "    for chunk in long_chunks[:3]:\n",
    "        print(f\"    - {chunk['tokens']} tokens from: {chunk['metadata']['title'][:60]}\")\n",
    "\n",
    "# Empty content\n",
    "empty_chunks = [c for c in all_chunks if not c['content'].strip()]\n",
    "print(f\"\\n‚ö†Ô∏è  Empty chunks: {len(empty_chunks)}\")\n",
    "\n",
    "# Optimal range (300-900 tokens)\n",
    "optimal_chunks = [c for c in all_chunks if 300 <= c['tokens'] <= 900]\n",
    "print(f\"\\n‚úì Optimal chunks (300-900 tokens): {len(optimal_chunks)} ({len(optimal_chunks)/len(all_chunks)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f10f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING CHUNKS\n",
      "======================================================================\n",
      "‚úì Saved 1747 chunks to: chunks.json\n",
      "‚úì Saved to: chunks.jsonl\n",
      "‚úì Saved metadata to: chunks_metadata.json\n",
      "‚úì Saved preview to: chunks_preview.csv\n",
      "\n",
      "======================================================================\n",
      "FILES CREATED:\n",
      "======================================================================\n",
      "  1. chunks.json - All chunks with full metadata\n",
      "  2. chunks.jsonl - Same data, one per line (efficient)\n",
      "  3. chunks_metadata.json - Statistics and config\n",
      "  4. chunks_preview.csv - Easy viewing in Excel\n",
      "\n",
      "Ready for embedding generation! üöÄ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING CHUNKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save as JSON\n",
    "output_file = 'chunks.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved {len(all_chunks)} chunks to: {output_file}\")\n",
    "\n",
    "# Also save as JSONL (one chunk per line - efficient for large datasets)\n",
    "output_jsonl = 'chunks.jsonl'\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"‚úì Saved to: {output_jsonl}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_documents': len(data),\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'avg_chunks_per_doc': len(all_chunks) / len(data),\n",
    "    'chunk_size': 800,\n",
    "    'chunk_overlap': 150,\n",
    "    'avg_tokens_per_chunk': statistics.mean(chunk_tokens),\n",
    "    'languages': lang_dist,\n",
    "    'sections': section_dist,\n",
    "}\n",
    "\n",
    "with open('chunks_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úì Saved metadata to: chunks_metadata.json\")\n",
    "\n",
    "# Save as CSV for easy viewing\n",
    "chunks_df = pd.DataFrame([\n",
    "    {\n",
    "        'chunk_id': c['chunk_id'],\n",
    "        'chunk_index': c['chunk_index'],\n",
    "        'tokens': c['tokens'],\n",
    "        'language': c['metadata']['language'],\n",
    "        'section': c['metadata']['section'],\n",
    "        'title': c['metadata']['title'],\n",
    "        'url': c['metadata']['url'],\n",
    "        'content_preview': c['content'][:200] + '...'\n",
    "    }\n",
    "    for c in all_chunks\n",
    "])\n",
    "\n",
    "chunks_df.to_csv('chunks_preview.csv', index=False)\n",
    "print(f\"‚úì Saved preview to: chunks_preview.csv\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FILES CREATED:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"  1. chunks.json - All chunks with full metadata\")\n",
    "print(\"  2. chunks.jsonl - Same data, one per line (efficient)\")\n",
    "print(\"  3. chunks_metadata.json - Statistics and config\")\n",
    "print(\"  4. chunks_preview.csv - Easy viewing in Excel\")\n",
    "print(f\"\\nReady for embedding generation! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b39b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHUNKING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Input:\n",
      "   Documents: 723\n",
      "\n",
      "üì¶ Output:\n",
      "   Total chunks: 1747\n",
      "   Avg chunks/doc: 2.4\n",
      "\n",
      "üéØ Quality:\n",
      "   Avg tokens/chunk: 585\n",
      "   Optimal chunks: 83.3%\n",
      "\n",
      "üåç Languages:\n",
      "   en: 545 chunks\n",
      "   tr: 1202 chunks\n",
      "\n",
      "‚úÖ Next Step: Generate embeddings using chunks.json or chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Input:\")\n",
    "print(f\"   Documents: {len(data)}\")\n",
    "\n",
    "print(f\"\\nüì¶ Output:\")\n",
    "print(f\"   Total chunks: {len(all_chunks)}\")\n",
    "print(f\"   Avg chunks/doc: {len(all_chunks)/len(data):.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ Quality:\")\n",
    "print(f\"   Avg tokens/chunk: {statistics.mean(chunk_tokens):.0f}\")\n",
    "print(f\"   Optimal chunks: {len(optimal_chunks)/len(all_chunks)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüåç Languages:\")\n",
    "for lang, count in lang_dist.items():\n",
    "    print(f\"   {lang}: {count} chunks\")\n",
    "\n",
    "print(f\"\\n‚úÖ Next Step: Generate embeddings using chunks.json or chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0b826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
