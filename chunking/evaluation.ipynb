{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Comprehensive Evaluation\\n",
        "\\n",
        "This notebook evaluates the IZU RAG chatbot system on 50 test questions.\\n",
        "\\n",
        "## Metrics Evaluated:\\n",
        "- **Retrieval Quality**: URL coverage, topic coverage, relevance\\n",
        "- **Answer Quality**: Semantic similarity, keyword overlap\\n",
        "- **Performance**: Response time, throughput\\n",
        "- **Cost**: Token usage, API costs\\n",
        "\\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\\n",
        "import numpy as np\\n",
        "import faiss\\n",
        "import openai\\n",
        "from dotenv import load_dotenv\\n",
        "import os\\n",
        "import time\\n",
        "from datetime import datetime\\n",
        "import pandas as pd\\n",
        "from tqdm import tqdm\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "from sklearn.metrics.pairwise import cosine_similarity\\n",
        "\\n",
        "# Setup\\n",
        "load_dotenv()\\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\\n",
        "\\n",
        "print(\\\"\u2713 Imports loaded\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"Loading RAG system resources...\\\")\\n",
        "\\n",
        "# Load chunks\\n",
        "with open('chunks.json', 'r', encoding='utf-8') as f:\\n",
        "    chunks = json.load(f)\\n",
        "print(f\\\"\u2713 Loaded {len(chunks)} chunks\\\")\\n",
        "\\n",
        "# Load FAISS index\\n",
        "index = faiss.read_index('faiss_index.bin')\\n",
        "print(f\\\"\u2713 Loaded FAISS index: {index.ntotal} vectors\\\")\\n",
        "\\n",
        "# Load F test dataset\\n",
        "with open('test_dataset.json', 'r', encoding='utf-8') as f:\\n",
        "    test_questions = json.load(f)\\n",
        "print(f\\\"\u2713 Loaded {len(test_questions)} test questions\\\")\\n",
        "\\n",
        "print(f\\\"\\\\n{'='*60}\\\")\\n",
        "print(\\\"SYSTEM READY FOR EVALUATION\\\")\\n",
        "print(f\\\"{'='*60}\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(text):\\n",
        "    \\\"\\\"\\\"Get embedding for text\\\"\\\"\\\"\\n",
        "    response = openai.embeddings.create(\\n",
        "        input=[text.replace(\\\"\\\\n\\\", \\\" \\\")],\\n",
        "        model=\\\"text-embedding-3-small\\\"\\n",
        "    )\\n",
        "    return response.data[0].embedding\\n",
        "\\n",
        "def retrieve_chunks(query, top_k=5):\\n",
        "    \\\"\\\"\\\"Retrieve most relevant chunks\\\"\\\"\\\"\\n",
        "    query_embedding = np.array([get_embedding(query)], dtype='float32')\\n",
        "    faiss.normalize_L2(query_embedding)\\n",
        "    scores, indices = index.search(query_embedding, top_k)\\n",
        "    \\n",
        "    results = []\\n",
        "    for idx, score in zip(indices[0], scores[0]):\\n",
        "        results.append({\\n",
        "            'content': chunks[idx]['content'],\\n",
        "            'metadata': chunks[idx]['metadata'],\\n",
        "            'score': float(score)\\n",
        "        })\\n",
        "    return results\\n",
        "\\n",
        "def answer_question(query, top_k=5):\\n",
        "    \\\"\\\"\\\"Answer question using RAG\\\"\\\"\\\"\\n",
        "    start_time = time.time()\\n",
        "    \\n",
        "    # Retrieve\\n",
        "    retrieval_start = time.time()\\n",
        "    retrieved = retrieve_chunks(query, top_k)\\n",
        "    retrieval_time = time.time() - retrieval_start\\n",
        "    \\n",
        "    # Build context\\n",
        "    context = \\\"\\\\n---\\\\n\\\".join([\\n",
        "        f\\\"Kaynak: {c['metadata']['title']}\\\\n{c['content']}\\\"\\n",
        "        for c in retrieved\\n",
        "    ])\\n",
        "    \\n",
        "    # Generate\\n",
        "    generation_start = time.time()\\n",
        "    response = openai.chat.completions.create(\\n",
        "        model=\\\"gpt-4o-mini\\\",\\n",
        "        messages=[\\n",
        "            {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Sen \u0130Z\u00dc i\u00e7in bir asistans\u0131n. Sadece verilen bilgileri kullan. T\u00fcrk\u00e7e cevap ver.\\\"},\\n",
        "            {\\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Context:\\\\n{context}\\\\n\\\\nSoru: {query}\\\"}\\n",
        "        ],\\n",
        "        temperature=0.3,\\n",
        "        max_tokens=500\\n",
        "    )\\n",
        "    generation_time = time.time() - generation_start\\n",
        "    total_time = time.time() - start_time\\n",
        "    \\n",
        "    return {\\n",
        "        'answer': response.choices[0].message.content,\\n",
        "        'retrieved_chunks': retrieved,\\n",
        "        'retrieval_time': retrieval_time,\\n",
        "        'generation_time': generation_time,\\n",
        "        'total_time': total_time,\\n",
        "        'tokens_used': response.usage.total_tokens,\\n",
        "        'prompt_tokens': response.usage.prompt_tokens,\\n",
        "        'completion_tokens': response.usage.completion_tokens\\n",
        "    }\\n",
        "\\n",
        "print(\\\"\u2713 RAG functions defined\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_semantic_similarity(text1, text2):\\n",
        "    \\\"\\\"\\\"Calculate semantic similarity between two texts\\\"\\\"\\\"\\n",
        "    emb1 = np.array(get_embedding(text1))\\n",
        "    emb2 = np.array(get_embedding(text2))\\n",
        "    return float(cosine_similarity([emb1], [emb2])[0][0])\\n",
        "\\n",
        "def calculate_keyword_overlap(text1, text2):\\n",
        "    \\\"\\\"\\\"Calculate keyword overlap between texts\\\"\\\"\\\"\\n",
        "    words1 = set(text1.lower().split())\\n",
        "    words2 = set(text2.lower().split())\\n",
        "    if not words1 or not words2:\\n",
        "        return 0.0\\n",
        "    intersection = words1.intersection(words2)\\n",
        "    union = words1.union(words2)\\n",
        "    return len(intersection) / len(union)\\n",
        "\\n",
        "def evaluate_retrieval(retrieved_chunks, expected_topics, expected_urls):\\n",
        "    \\\"\\\"\\\"Evaluate retrieval quality\\\"\\\"\\\"\\n",
        "    # URL coverage\\n",
        "    retrieved_urls = [c['metadata']['url'] for c in retrieved_chunks]\\n",
        "    url_matches = sum(1 for url in expected_urls if any(exp in url for exp in retrieved_urls))\\n",
        "    url_coverage = url_matches / len(expected_urls) if expected_urls else 0\\n",
        "    \\n",
        "    # Topic coverage\\n",
        "    all_content = ' '.join([c['content'].lower() for c in retrieved_chunks])\\n",
        "    topic_matches = sum(1 for topic in expected_topics if topic.lower() in all_content)\\n",
        "    topic_coverage = topic_matches / len(expected_topics) if expected_topics else 0\\n",
        "    \\n",
        "    # Average relevance score\\n",
        "    avg_score = np.mean([c['score'] for c in retrieved_chunks]) if retrieved_chunks else 0\\n",
        "    \\n",
        "    return {\\n",
        "        'url_coverage': url_coverage,\\n",
        "        'topic_coverage': topic_coverage,\\n",
        "        'avg_relevance_score': float(avg_score)\\n",
        "    }\\n",
        "\\n",
        "print(\\\"\u2713 Evaluation functions defined\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"=\"*80)\\n",
        "print(\\\"RUNNING COMPREHENSIVE EVALUATION\\\")\\n",
        "print(\\\"=\"*80)\\n",
        "print()\\n",
        "\\n",
        "results = []\\n",
        "\\n",
        "for i, question_data in enumerate(tqdm(test_questions, desc=\\\"Evaluating\\\"), 1):\\n",
        "    try:\\n",
        "        # Get question\\n",
        "        question = question_data['question_tr']\\n",
        "        \\n",
        "        # Get answer\\n",
        "        result = answer_question(question)\\n",
        "        \\n",
        "        # Evaluate retrieval\\n",
        "        retrieval_metrics = evaluate_retrieval(\\n",
        "            result['retrieved_chunks'],\\n",
        "            question_data.get('expected_topics', []),\\n",
        "            question_data.get('requires_urls', [])\\n",
        "        )\\n",
        "        \\n",
        "        # Evaluate answer quality\\n",
        "        semantic_sim = calculate_semantic_similarity(\\n",
        "            result['answer'],\\n",
        "            question_data.get('ground_truth_answer', '')\\n",
        "        )\\n",
        "        \\n",
        "        keyword_overlap = calculate_keyword_overlap(\\n",
        "            result['answer'],\\n",
        "            question_data.get('ground_truth_answer', '')\\n",
        "        )\\n",
        "        \\n",
        "        # Calculate cost (GPT-4o-mini pricing)\\n",
        "        cost = (result['prompt_tokens'] * 0.15 / 1_000_000) + \\\\\\n",
        "               (result['completion_tokens'] * 0.6 / 1_000_000)\\n",
        "        \\n",
        "        # Store results\\n",
        "        results.append({\\n",
        "            'question_id': question_data['id'],\\n",
        "            'question': question,\\n",
        "            'category': question_data['category'],\\n",
        "            'difficulty': question_data['difficulty'],\\n",
        "            'answer': result['answer'],\\n",
        "            'ground_truth': question_data.get('ground_truth_answer', ''),\\n",
        "            'semantic_similarity': semantic_sim,\\n",
        "            'keyword_overlap': keyword_overlap,\\n",
        "            'url_coverage': retrieval_metrics['url_coverage'],\\n",
        "            'topic_coverage': retrieval_metrics['topic_coverage'],\\n",
        "            'avg_relevance_score': retrieval_metrics['avg_relevance_score'],\\n",
        "            'total_time_ms': result['total_time'] * 1000,\\n",
        "            'retrieval_time_ms': result['retrieval_time'] * 1000,\\n",
        "            'generation_time_ms': result['generation_time'] * 1000,\\n",
        "            'tokens_used': result['tokens_used'],\\n",
        "            'cost_usd': cost\\n",
        "        })\\n",
        "        \\n",
        "    except Exception as e:\\n",
        "        print(f\\\"\\\\nError on question {i}: {e}\\\")\\n",
        "        continue\\n",
        "\\n",
        "# Convert to DataFrame\\n",
        "df = pd.DataFrame(results)\\n",
        "\\n",
        "print(f\\\"\\\\n\u2713 Evaluation complete!\\\")\\n",
        "print(f\\\"\u2713 Evaluated {len(results)} questions\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"=\"*80)\\n",
        "print(\\\"OVERALL METRICS\\\")\\n",
        "print(\\\"=\"*80)\\n",
        "print()\\n",
        "\\n",
        "print(\\\"Answer Quality:\\\")\\n",
        "print(f\\\"  Semantic Similarity: {df['semantic_similarity'].mean():.3f}\\\")\\n",
        "print(f\\\"  Keyword Overlap: {df['keyword_overlap'].mean():.3f}\\\")\\n",
        "print()\\n",
        "\\n",
        "print(\\\"Retrieval Quality:\\\")\\n",
        "print(f\\\"  URL Coverage: {df['url_coverage'].mean():.1%}\\\")\\n",
        "print(f\\\"  Topic Coverage: {df['topic_coverage'].mean():.1%}\\\")\\n",
        "print(f\\\"  Avg Relevance Score: {df['avg_relevance_score'].mean():.3f}\\\")\\n",
        "print()\\n",
        "\\n",
        "print(\\\"Performance:\\\")\\n",
        "print(f\\\"  Avg Total Time: {df['total_time_ms'].mean():.0f}ms\\\")\\n",
        "print(f\\\"  Avg Retrieval Time: {df['retrieval_time_ms'].mean():.0f}ms\\\")\\n",
        "print(f\\\"  Avg Generation Time: {df['generation_time_ms'].mean():.0f}ms\\\")\\n",
        "print(f\\\"  95th Percentile: {df['total_time_ms'].quantile(0.95):.0f}ms\\\")\\n",
        "print()\\n",
        "\\n",
        "print(\\\"Cost:\\\")\\n",
        "print(f\\\"  Avg Cost per Question: ${df['cost_usd'].mean():.4f}\\\")\\n",
        "print(f\\\"  Total Cost: ${df['cost_usd'].sum():.2f}\\\")\\n",
        "print(f\\\"  Avg Tokens: {df['tokens_used'].mean():.0f}\\\")\\n",
        "print()\\n",
        "\\n",
        "print(\\\"Projected Costs:\\\")\\n",
        "print(f\\\"  1,000 questions: ${df['cost_usd'].mean() * 1000:.2f}\\\")\\n",
        "print(f\\\"  10,000 questions: ${df['cost_usd'].mean() * 10000:.2f}\\\")\\n",
        "print(f\\\"  100,000 questions: ${df['cost_usd'].mean() * 100000:.2f}\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"=\"*80)\\n",
        "print(\\\"PERFORMANCE BY CATEGORY\\\")\\n",
        "print(\\\"=\"*80)\\n",
        "print()\\n",
        "\\n",
        "category_stats = df.groupby('category').agg({\\n",
        "    'semantic_similarity': 'mean',\\n",
        "    'keyword_overlap': 'mean',\\n",
        "    'total_time_ms': 'mean',\\n",
        "    'cost_usd': 'mean'\\n",
        "}).round(3)\\n",
        "\\n",
        "print(category_stats.to_string())\\n",
        "print()\\n",
        "\\n",
        "# Best and worst categories\\n",
        "best_cat = category_stats['semantic_similarity'].idxmax()\\n",
        "worst_cat = category_stats['semantic_similarity'].idxmin()\\n",
        "\\n",
        "print(f\\\"Best Category: {best_cat} ({category_stats.loc[best_cat, 'semantic_similarity']:.3f})\\\")\\n",
        "print(f\\\"Worst Category: {worst_cat} ({category_stats.loc[worst_cat, 'semantic_similarity']:.3f})\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup plotting\\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n",
        "\\n",
        "# 1. Semantic Similarity by Category\\n",
        "df.groupby('category')['semantic_similarity'].mean().sort_values().plot(kind='barh', ax=axes[0,0], color='skyblue')\\n",
        "axes[0,0].set_title('Semantic Similarity by Category')\\n",
        "axes[0,0].set_xlabel('Similarity Score')\\n",
        "axes[0,0].axvline(0.6, color='green', linestyle='--', label='Good (>0.6)')\\n",
        "axes[0,0].legend()\\n",
        "\\n",
        "# 2. Response Time Distribution\\n",
        "axes[0,1].hist(df['total_time_ms'], bins=20, color='lightcoral', edgecolor='black')\\n",
        "axes[0,1].set_title('Response Time Distribution')\\n",
        "axes[0,1].set_xlabel('Time (ms)')\\n",
        "axes[0,1].set_ylabel('Frequency')\\n",
        "axes[0,1].axvline(3000, color='green', linestyle='--', label='Target (<3s)')\\n",
        "axes[0,1].legend()\\n",
        "\\n",
        "# 3. Performance by Difficulty\\n",
        "df.groupby('difficulty')['semantic_similarity'].mean().plot(kind='bar', ax=axes[1,0], color='lightgreen')\\n",
        "axes[1,0].set_title('Performance by Difficulty')\\n",
        "axes[1,0].set_ylabel('Semantic Similarity')\\n",
        "axes[1,0].set_xticklabels(axes[1,0].get_xticklabels(), rotation=0)\\n",
        "\\n",
        "# 4. Cost vs Performance\\n",
        "axes[1,1].scatter(df['cost_usd'] * 1000, df['semantic_similarity'], alpha=0.6, color='purple')\\n",
        "axes[1,1].set_title('Cost vs Performance')\\n",
        "axes[1,1].set_xlabel('Cost (cents)')\\n",
        "axes[1,1].set_ylabel('Semantic Similarity')\\n",
        "axes[1,1].grid(True, alpha=0.3)\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.savefig('evaluation_dashboard.png', dpi=150, bbox_inches='tight')\\n",
        "plt.show()\\n",
        "\\n",
        "print(\\\"\u2713 Dashboard saved: evaluation_dashboard.png\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"=\"*80)\\n",
        "print(\\\"SAMPLE RESULTS\\\")\\n",
        "print(\\\"=\"*80)\\n",
        "print()\\n",
        "\\n",
        "# Show best and worst examples\\n",
        "best_idx = df['semantic_similarity'].idxmax()\\n",
        "worst_idx = df['semantic_similarity'].idxmin()\\n",
        "\\n",
        "print(\\\"BEST RESULT:\\\")\\n",
        "print(f\\\"Question: {df.loc[best_idx, 'question']}\\\")\\n",
        "print(f\\\"Similarity: {df.loc[best_idx, 'semantic_similarity']:.3f}\\\")\\n",
        "print(f\\\"Answer: {df.loc[best_idx, 'answer'][:200]}...\\\")\\n",
        "print()\\n",
        "\\n",
        "print(\\\"WORST RESULT:\\\")\\n",
        "print(f\\\"Question: {df.loc[worst_idx, 'question']}\\\")\\n",
        "print(f\\\"Similarity: {df.loc[worst_idx, 'semantic_similarity']:.3f}\\\")\\n",
        "print(f\\\"Answer: {df.loc[worst_idx, 'answer'][:200]}...\\\")\\n",
        "print(f\\\"Ground Truth: {df.loc[worst_idx, 'ground_truth'][:200]}...\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\\n",
        "\\n",
        "# CSV\\n",
        "csv_file = f'evaluation_results_{timestamp}.csv'\\n",
        "df.to_csv(csv_file, index=False)\\n",
        "print(f\\\"\u2713 Saved: {csv_file}\\\")\\n",
        "\\n",
        "# JSON\\n",
        "json_file = f'evaluation_results_{timestamp}.json'\\n",
        "df.to_json(json_file, orient='records', indent=2)\\n",
        "print(f\\\"\u2713 Saved: {json_file}\\\")\\n",
        "\\n",
        "# Summary report\\n",
        "report = f\\\"\\\"\\\"\\n",
        "# RAG Evaluation Report\\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n",
        "\\n",
        "## Overall Metrics\\n",
        "- Semantic Similarity: {df['semantic_similarity'].mean():.3f}\\n",
        "- URL Coverage: {df['url_coverage'].mean():.1%}\\n",
        "- Avg Response Time: {df['total_time_ms'].mean():.0f}ms\\n",
        "- Avg Cost: ${df['cost_usd'].mean():.4f}\\n",
        "\\n",
        "## Performance by Category\\n",
        "{category_stats.to_markdown()}\\n",
        "\\n",
        "## Status\\n",
        "{'PASS: System meets quality targets' if df['semantic_similarity'].mean() > 0.6 else 'FAIL: System below quality targets'}\\n",
        "\\\"\\\"\\\"\\n",
        "\\n",
        "report_file = f'evaluation_report_{timestamp}.md'\\n",
        "with open(report_file, 'w') as f:\\n",
        "    f.write(report)\\n",
        "\\n",
        "print(f\\\"\u2713 Saved: {report_file}\\\")\\n",
        "print()\\n",
        "print(\\\"=\"*80)\\n",
        "print(\\\"EVALUATION COMPLETE\\\")\\n",
        "print(\\\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}